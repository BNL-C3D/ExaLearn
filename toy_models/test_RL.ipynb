{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import itertools\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = state_space \n",
    "        self.action_space = action_space\n",
    "        self.l1 = nn.Sequential(nn.Linear(self.state_space, 6*self.state_space), nn.ReLU())\n",
    "        self.l2 = nn.Sequential(nn.Linear(6*self.state_space, 10*self.action_space), nn.ReLU())\n",
    "        self.l3 = nn.Linear(10*self.action_space, self.action_space)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERenv():\n",
    "    import platform\n",
    "    import numpy as np\n",
    "    def __init__(self, exe, init_m = 100, nrange=(100,200), mc=10):\n",
    "        \"\"\"\n",
    "            nrange is half-open [a,b)\n",
    "        \"\"\"\n",
    "        self.exe    = exe\n",
    "        self.mc     = mc\n",
    "        self.nrange = (100,200)\n",
    "        self.init_m = init_m\n",
    "        self.state  = [np.random.randint(*self.nrange), init_m]\n",
    "        # self.pyver  = platform.python_version()[:3] == 3.6\n",
    "        \n",
    "    def step(self, p):\n",
    "        if True: \n",
    "            tmp = subprocess.run(['./main', str(self.state[0]), str(10) ,str(p)],stdout=subprocess.PIPE) ## 3.6\n",
    "        else:\n",
    "            tmp = subprocess.run(['./main', str(self.state[0]), str(10) ,str(p)],capture_output=True) ## 3.7\n",
    "        res = float(tmp.stdout.split()[0])\n",
    "        self.state[0] = np.random.randint(*self.nrange)\n",
    "        self.state[1] -= 1\n",
    "        return self.state.copy(), float(tmp.stdout.split()[0]), self.state[1]==0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state[0] = np.random.randint(*self.nrange)\n",
    "        self.state[1] = self.init_m\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def run(self, state, p):\n",
    "        tmp = subprocess.run(['./main', str(state[0]), str(10) ,str(p)],stdout=subprocess.PIPE) ## 3.6\n",
    "        return float(tmp.stdout.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simu(env, policy_net, batch_size=8, gamma=0.99):\n",
    "    state_pool, action_pool, reward_pool = [], [], []\n",
    "    for b in range(batch_size):\n",
    "        done  = False\n",
    "        state = env.reset()\n",
    "        cur_reward_pool = []\n",
    "        for t in itertools.count():\n",
    "            if done: break\n",
    "            state_pool.append(state)\n",
    "            state  = torch.tensor(state).float()\n",
    "            action = policy_net(state)\n",
    "            action_pool.append(action)\n",
    "            state, reward, done, *_ = env.step(nn.Sigmoid()(action).item())\n",
    "            cur_reward_pool.append(reward)\n",
    "        \n",
    "        # discount reward\n",
    "        running_mean = 0\n",
    "        for i in reversed(range(len(cur_reward_pool))):\n",
    "            running_mean = running_mean*gamma + cur_reward_pool[i]\n",
    "            cur_reward_pool[i] = running_mean\n",
    "        reward_pool.extend(cur_reward_pool)\n",
    "    \n",
    "    # normalize reward\n",
    "    avg, std = np.mean(reward_pool), np.std(reward_pool)\n",
    "    reward_pool = list(map(lambda x: (x-avg)/std, reward_pool))\n",
    "    \n",
    "    return state_pool, action_pool, reward_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy_net, optim, state_pool, action_pool, reward_pool):\n",
    "    optim.zero_grad()\n",
    "    for s, a, r in zip(state_pool, action_pool, reward_pool):\n",
    "        m    = torch.distributions.Normal(nn.Sigmoid()(a), torch.tensor([0.0001]))\n",
    "        loss = -m.log_prob(a)*r\n",
    "        loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ERenv('./main', init_m=10)\n",
    "policy_net = Policy(2, 1)\n",
    "optim = torch.optim.SGD(policy_net.parameters(), lr=0.001)\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    tmp_act = policy_net(torch.tensor([155, 5]).float()).sigmoid().item()\n",
    "    print( tmp_act, env.run([155,5], tmp_act) )\n",
    "    update_policy(policy_net, optim, *simu(env, policy_net, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
